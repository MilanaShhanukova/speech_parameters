{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "In this notebook we will explain the details of the evaluation pipeline suggested. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One metric evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture: NISQA_DIM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.0197115"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subjective.mos_metric import MOSMetric\n",
    "\n",
    "audio_path_example = './dataset/sample.wav'\n",
    "\n",
    "mos_metric = MOSMetric()\n",
    "mos_metric.score_audio(audio_path_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the results over the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can also get the results over the whole dataset. As long as most of the time we work over the csv files, we can pass the csv file with two columns that include the audio files that we want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_stats import process_audio_files\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['HF_TOKEN'] = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(audio_path_example)\n",
    "\n",
    "noise_factor = 0.008\n",
    "\n",
    "noise = torch.randn(waveform.size()) * noise_factor\n",
    "noisy_waveform = waveform + noise\n",
    "\n",
    "noisy_waveform = torch.clamp(noisy_waveform, -1.0, 1.0)\n",
    "\n",
    "noisy_audio_path = audio_path_example.replace(\".wav\", \"_noisy.wav\")\n",
    "torchaudio.save(noisy_audio_path, noisy_waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mount/studenten-temp1/users/milana/speech_parameters/dataset/sample.wav /mount/studenten-temp1/users/milana/speech_parameters/dataset/sample_noisy.wav\n",
      "Model architecture: NISQA_DIM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.6.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../home/users1/shkhanma/.cache/torch/pyannote/models--pyannote--brouhaha/snapshots/c93c9b537732dd50c28c0366c73f560c3a7aeb02/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.12.1+cu102, yours is 2.4.0+cu121. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.6.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../home/users1/shkhanma/.cache/torch/pyannote/models--pyannote--brouhaha/snapshots/c93c9b537732dd50c28c0366c73f560c3a7aeb02/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.12.1+cu102, yours is 2.4.0+cu121. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 22.49it/s]\n",
      "1it [00:01,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./dataset/result_sample.csv\n"
     ]
    }
   ],
   "source": [
    "process_audio_files(\n",
    "    clean_audio_paths=[audio_path_example],\n",
    "    dirty_audio_paths=[noisy_audio_path],\n",
    "    metrics=[\"STOI\", \"MOS\"],\n",
    "    output_file='./dataset/result_sample.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
